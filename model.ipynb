{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the token to download the model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\personal\\timepass\\assignment\\my_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Quantization of model to reduce the inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear}, \n",
    "    dtype=torch.qint8  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction funtion to get the next n tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_next_tokens_with_cache(model, tokenizer, input_text, num_tokens=10):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=input_ids.shape[1] + num_tokens,\n",
    "        do_sample=True,  \n",
    "        temperature=0.5, \n",
    "        top_k=30, \n",
    "        top_p=0.95, \n",
    "        repetition_penalty=1.5, \n",
    "        num_return_sequences=1 \n",
    "    )\n",
    "\n",
    "    # Decode the generated sequence\n",
    "    generated_sequence = outputs[0]\n",
    "    decoded_sequence = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict next token to get the generated text and calculate the average inference time per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "input_text = \"India is the best country is world because\"\n",
    "num_tokens=100\n",
    "generated_text = predict_next_tokens_with_cache(model, tokenizer, input_text, num_tokens=num_tokens)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: India is the best country is world because of its rich culture, beautiful natural beauty and a large population. India has become one among many countries which have been recognized by UNESCO as heritage sites in Asia.\n",
      "The Indian government also declared 12 cities across various states to be global city under Smart Cities Mission (SCM) scheme for improving urban infrastructure through innovative solutions such that they can offer better quality services at affordable prices while providing sustainable development benefits like economic growth & employment generation etc..\n",
      "In this article we will discuss about some popular smart or digital tourist\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference time: 0.1942 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Inference time: {(end_time - start_time)/num_tokens:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
